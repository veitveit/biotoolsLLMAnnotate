pipeline:
  input_path: null
  payload_version: "__VERSION__"
  from_date: "2025-01-02"
  to_date: "2025-01-03"
  resume_from_pub2tools: true
  resume_from_enriched: true
  resume_from_scoring: false
  min_bio_score: 0.6
  min_documentation_score: 0.6
pub2tools:
  edam_owl: http://edamontology.org/EDAM.owl
  idf: https://github.com/edamontology/edammap/raw/master/doc/biotools.idf
  idf_stemmed: https://github.com/edamontology/edammap/raw/master/doc/biotools.stemmed.idf
  firefox_path: null
  p2t_cli: bin/pub2tools
  custom_restriction: SRC:MED OR SRC:PMC
  disable_tool_restriction: true
  timeout: 6000
  retryLimit: 0
  fetcher_threads: 4
enrichment:
  europe_pmc:
    enabled: true
    include_full_text: true
    max_publications: 1
    max_full_text_chars: 4000
    timeout: 15
  homepage:
    enabled: true
    timeout: 20
  user_agent: biotoolsllmannotate/__VERSION__ (+https://github.com/ELIXIR-Belgium/biotoolsLLMAnnotate)
ollama:
  host: http://localhost:11434
  model: llama3.2
  max_retries: 6
  retry_backoff_seconds: 2
  concurrency: 4
logging:
  level: INFO
  file: null
scoring_prompt_template: |
  You are evaluating whether a software resource is worth getting registered in bio.tools, the registry for software resources in the life sciences.

  Available material:

  Title: {title}
  Description: {description}
  Homepage: {homepage}
  Homepage status: {homepage_status}
  Homepage error: {homepage_error}
  Documentation links: {documentation}
  Documentation keywords found on homepage: {documentation_keywords}
  Repository: {repository}
  Found keywords: {tags}
  Published: {published_at}
  Publication abstract: {publication_abstract}
  Publication full text: {publication_full_text}
  Known publication identifiers: {publication_ids}

  Note: The documentation keywords and found keywords listed above were automatically mined from the homepage, papers, and other reference material whose raw text is not included in this prompt. Treat them as secondary hints and cite them explicitly when used.

  Task:
  Score the resource using the rubric below. For every subcriterion assign exactly one of {{0, 0.5, 1}}. Base every decision only on the provided material. Do not invent facts or URLs. If the resource is not life-science software, set ALL bio subcriteria A1–A5 = 0 and explain why in the rationale.

  Bio score rubric
  A1 Biological intent stated (explicit life-science task/domain).
  A2 Operations on biological data described
  A3 Software with biological data I/O: 0 = none; 0.5 = only generic; 1 = concrete datatypes/formats named.
  A4 Modality explicitly classifiable as one or more of: database portal, desktop application, web application, web API, web service, SPARQL endpoint, command-line tool (CLI), workbench, suite, plug-in, workflow, library, ontology. Include minimal usage context.
  A5 Evidence of bio use (examples on real bio data OR peer-reviewed/benchmark citation).

  Documentation score rubric (subcriteria only; no overall score here)
  B1 Documentation completeness (e.g. manual, guide, readthedocs).
  B2 Installation pathways (e.g. installation/setup, config, container, package).
  B3 Reproducibility aids (e.g. doi, release).
  B4 Maintenance signal (e.g. commits, issue tracker, news).
  B5 Onboarding & support (e.g. quickstart/tutorial, contact, faq).

  Selection/normalization rules:

  Base every decision on the supplied material only.
  Documentation links and keywords listed above are rubric-aligned evidence sources. Interpret them as:
  • Installation pathways (B2): presence of urls or keywords such as install, installation, package, pip, conda, workflow → award ≥0.5; reach 1.0 when both keyword and explicit link are provided.
  • Reproducibility aids (B3): release, version, tag, news, changelog in links/keywords → award ≥0.5; raise to 1.0 when multiple supporting signals exist.
  • Maintenance signal (B4): commit, issues, activity, support, community, workflow → award ≥0.5; raise to 1.0 if both keyword and corresponding link (e.g. commits page) are present.
  • Documentation completeness & onboarding (B1/B5): doc, docs, documentation, guide, manual, usage, faq, contact → award ≥0.5; raise to 1.0 when links confirm comprehensive material.
  Explicitly cite the supporting keyword or link as "keyword evidence" when scoring.
  Tags listed above are likewise extracted hints from the same external sources; cite them as "keyword evidence" when relevant but do not infer new facts beyond what is stated.
  Normalize publication identifiers to prefixes: DOI:..., PMID:..., PMCID:... and remove duplicates (case-insensitive).
  For any subcriterion scored 0 due to missing evidence, mention "insufficient evidence: <item>" in the rationale.
  Record each bio subcriterion as numbers {{0,0.5,1}} in `bio_subscores` and each documentation subcriterion as numbers {{0,0.5,1}} in `documentation_subscores`.
  Do NOT compute aggregate scores; only fill the provided fields.
  Do not output any value outside [0.0, 1.0].
  Always emit every field in the output JSON exactly once.
  Emit ONLY the fields in the schema below. Use "" for unknown strings and [] if no publication identifiers are found. Do not output booleans/strings instead of numbers.

  Output: respond ONLY with a single JSON object shaped as:
  {{
  "tool_name": "<derived display name>",
  "homepage": "<best homepage URL>",
  "publication_ids": ["DOI:...", "PMID:...", "PMCID:..."],
  "bio_subscores": {{"A1": <0|0.5|1>, "A2": <0|0.5|1>, "A3": <0|0.5|1>, "A4": <0|0.5|1>, "A5": <0|0.5|1>}},
  "documentation_subscores": {{"B1": <0|0.5|1>, "B2": <0|0.5|1>, "B3": <0|0.5|1>, "B4": <0|0.5|1>, "B5": <0|0.5|1>}},
  "concise_description": "<1–2 sentence rewritten summary>",
  "rationale": "<2–5 sentences citing specific evidence for both score groups; for each claim indicate the source as one of: homepage, documentation, repository, abstract, full_text, tags; explicitly name missing items as 'insufficient evidence: ...'>"
  }}

